import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime

# ==========================================
# è¨­å®š
# ==========================================
st.set_page_config(layout="wide", page_title="PTS & TDnet Monitor")
THRESHOLD_PERCENT = 3.0
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}

# ==========================================
# é–¢æ•°: PDFè¡¨ç¤ºç”¨ (Google Docs ViewerçµŒç”±)
# ==========================================
def display_pdf(url):
    """
    Google Docs Viewerã‚’ä½¿ç”¨ã—ã¦PDFã‚’è¡¨ç¤ºã™ã‚‹
    (TDnetã®iframeãƒ–ãƒ­ãƒƒã‚¯ã‚„Mixed Contentã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹ãŸã‚)
    """
    # Googleã®ãƒ“ãƒ¥ãƒ¼ã‚¢ã‚’çµŒç”±ã•ã›ã‚‹URLã‚’ä½œæˆ
    viewer_url = f"https://docs.google.com/viewer?url={url}&embedded=true"
    
    # iframeã§è¡¨ç¤º
    st.markdown(
        f'<iframe src="{viewer_url}" width="100%" height="800" frameborder="0"></iframe>',
        unsafe_allow_html=True
    )

# ==========================================
# é–¢æ•°: TDnetãƒ‡ãƒ¼ã‚¿å–å¾—
# ==========================================
@st.cache_data(ttl=300)
def get_todays_tdnet_data():
    date_str = datetime.now().strftime('%Y%m%d')
    base_url = "https://www.release.tdnet.info/inbs/I_list_{}_{}.html"
    root_url = "https://www.release.tdnet.info/inbs/"
    
    disclosure_map = {}
    page = 1
    
    while True:
        url = base_url.format(f"{page:03}", date_str)
        try:
            res = requests.get(url, headers=HEADERS, timeout=5)
            if res.status_code == 404: break
            
            res.encoding = 'utf-8' # æ–‡å­—åŒ–ã‘å¯¾ç­–
            
            soup = BeautifulSoup(res.text, 'html.parser')
            rows = soup.select("table tr")
            
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 4:
                    code_full = cols[1].text.strip()
                    code_4 = code_full[:4]
                    title = cols[3].text.strip()
                    
                    link_tag = cols[3].find('a')
                    pdf_url = ""
                    if link_tag and 'href' in link_tag.attrs:
                        pdf_url = root_url + link_tag['href']
                    
                    if code_4 not in disclosure_map:
                        disclosure_map[code_4] = []
                    
                    disclosure_map[code_4].append({
                        "time": cols[0].text.strip(),
                        "title": title,
                        "url": pdf_url
                    })
            page += 1
            time.sleep(0.1)
        except: break
    return disclosure_map

# ==========================================
# é–¢æ•°: PTSãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—
# ==========================================
@st.cache_data(ttl=60)
def get_ranking_data():
    candidates = []
    targets = [
        ("https://kabutan.jp/warning/pts_night_price_increase", "æ€¥é¨°"),
        ("https://kabutan.jp/warning/pts_night_price_decrease", "æ€¥è½")
    ]
    
    progress_text = "PTSãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."
    my_bar = st.progress(0, text=progress_text)
    
    for url, label in targets:
        try:
            res = requests.get(url, headers=HEADERS, timeout=10)
            res.encoding = res.apparent_encoding 
            
            soup = BeautifulSoup(res.text, 'html.parser')
            table = soup.select_one("table.stock_table")
            if not table: continue
            
            tbody = table.find("tbody")
            if tbody:
                rows = tbody.find_all("tr")
            else:
                rows = table.find_all("tr")[2:]
            
            for row in rows:
                cols = row.find_all(["td", "th"])
                if len(cols) < 10: continue
                
                try:
                    pct_str = cols[8].text.strip()
                    clean_pct = pct_str.replace("%", "").replace("+", "").replace(",", "")
                    if not clean_pct: continue
                    change_pct = float(clean_pct)
                    
                    if abs(change_pct) >= THRESHOLD_PERCENT:
                        code_tag = cols[0].find('a')
                        code = code_tag.text.strip() if code_tag else cols[0].text.strip()
                        name = cols[1].text.strip()
                        pts_price = cols[6].text.strip()
                        
                        candidates.append({
                            "Code": code,
                            "Name": name,
                            "PTS_Price": pts_price,
                            "Change_Pct": change_pct,
                            "Label": label
                        })
                except: continue
        except: continue
        
    my_bar.empty()
    return pd.DataFrame(candidates)

# ==========================================
# é–¢æ•°: æ—¥ä¸­4æœ¬å€¤
# ==========================================
def get_daily_ohlc(code):
    url = f"https://kabutan.jp/stock/?code={code}"
    d = {"Open": "-", "High": "-", "Low": "-", "Close": "-"}
    try:
        res = requests.get(url, headers=HEADERS, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        def find_val(label):
            th = soup.find("th", string=label)
            if th:
                td = th.find_next_sibling("td")
                if td: return td.text.strip()
            return "-"

        d["Open"] = find_val("å§‹å€¤")
        d["High"] = find_val("é«˜å€¤")
        d["Low"] = find_val("å®‰å€¤")
        d["Close"] = find_val("çµ‚å€¤")
        if d["Close"] == "-":
            span = soup.select_one("span.kabuka")
            if span: d["Close"] = span.text.strip()
        return d
    except: return d


# ==========================================
# ãƒ¡ã‚¤ãƒ³ç”»é¢
# ==========================================
st.title("ğŸ“Š PTSæ€¥å‹•æ„ & é©æ™‚é–‹ç¤ºãƒ¢ãƒ‹ã‚¿ãƒ¼")

with st.spinner('ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...'):
    tdnet_data = get_todays_tdnet_data()
    df_pts = get_ranking_data()

col_left, col_right = st.columns([1, 1]) 

with col_left:
    st.subheader("PTS ãƒ©ãƒ³ã‚­ãƒ³ã‚° (Â±3%ä»¥ä¸Š)")
    
    if not df_pts.empty:
        df_pts["News"] = df_pts["Code"].apply(lambda x: "ğŸ“„ã‚ã‚Š" if x in tdnet_data else "")
        display_df = df_pts[["Code", "Name", "PTS_Price", "Change_Pct", "News", "Label"]]
        
        event = st.dataframe(
            display_df.style.format({"Change_Pct": "{:.2f}%"}).map(
                lambda x: 'color: red;' if x < 0 else 'color: green;', subset=['Change_Pct']
            ),
            use_container_width=True,
            hide_index=True,
            on_select="rerun",
            selection_mode="single-row",
            height=600
        )
        
        selected_rows = event.selection.rows
        if selected_rows:
            idx = selected_rows[0]
            selected_code = display_df.iloc[idx]["Code"]
            selected_name = display_df.iloc[idx]["Name"]
        else:
            selected_code = None
    else:
        st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹éŠ˜æŸ„ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        selected_code = None

with col_right:
    st.subheader("è©³ç´° & é©æ™‚é–‹ç¤ºãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    
    if selected_code:
        st.markdown(f"### {selected_code} {selected_name}")
        
        with st.spinner('æ ªä¾¡è©³ç´°ã‚’å–å¾—ä¸­...'):
            ohlc = get_daily_ohlc(selected_code)
            
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("å§‹å€¤", ohlc["Open"])
        c2.metric("é«˜å€¤", ohlc["High"])
        c3.metric("å®‰å€¤", ohlc["Low"])
        c4.metric("çµ‚å€¤", ohlc["Close"])
        
        st.divider()

        if selected_code in tdnet_data:
            news_list = tdnet_data[selected_code]
            st.success(f"æœ¬æ—¥ {len(news_list)} ä»¶ã®é–‹ç¤ºãŒã‚ã‚Šã¾ã™")
            
            tabs = st.tabs([f"{n['time']} {n['title'][:10]}..." for n in news_list])
            
            for i, tab in enumerate(tabs):
                news = news_list[i]
                with tab:
                    st.markdown(f"**{news['title']}**")
                    if news['url']:
                        st.link_button("â†— åˆ¥ã‚¿ãƒ–ã§PDFã‚’é–‹ã", news['url'])
                        display_pdf(news['url']) # Google Viewerã§è¡¨ç¤º
                    else:
                        st.warning("PDFãƒªãƒ³ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            st.info("æœ¬æ—¥ã®é©æ™‚é–‹ç¤ºã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.markdown(f"â€¢ [Yahoo!æ²ç¤ºæ¿](https://finance.yahoo.co.jp/quote/{selected_code}.T/bbs)")
    else:
        st.info("ğŸ‘ˆ å·¦å´ã®è¡¨ã‹ã‚‰éŠ˜æŸ„ã‚’é¸æŠã—ã¦ãã ã•ã„")

st.divider()
if st.button("ãƒ‡ãƒ¼ã‚¿æ›´æ–°"):
    st.cache_data.clear()
    st.rerun()